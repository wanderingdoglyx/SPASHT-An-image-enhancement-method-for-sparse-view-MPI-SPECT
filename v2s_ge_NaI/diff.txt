109c109
< def val_step(model, x_batch_val, y_batch_val,y_batch_loc_val,y_batch_mask_val):
---
> def val_step(model, x_batch_val, y_batch_val,y_batch_loc_val, y_batch_mask_val):
120d119
< 
155c154,156
<   num_pat = 184
---
>   # train pat index (1185/5*0.8 = 189, 585/5*0.8=93)
>   # train pat index (1185/5*0.6 = 142, 585/5*0.8=69)
>   num_pat = 184 #change
196,197d196
<       #print(f'Ind: {ind_pat} || Pat id: {pat_id_arr[ind_pat]} || Sum train X: {np.sum(cur_X)} || Sum train Y: {np.sum(cur_Y)} || Rat: {np.sum(cur_X)/np.sum(cur_Y):.2f}')
<       #print(f'Ind: {ind_pat} || Pat id: {pat_id_arr[ind_pat]} || Rat: {np.sum(cur_X)/np.sum(cur_Y):.2f}')
206d204
< 
237d234
<             #print(f'Ind: {ind_pat} || Pat id: {pat_id_arr[ind_pat]} || Sum train X: {np.sum(X_data[j,:, :, :, :])} || Sum train Y: {np.sum(Y_data[j,:, :, :, :])}')
239a237,239
>   #sc_data = 1e-2
>   #X_data = X_data * sc_data
>   #Y_data = Y_data * sc_data
265,302c265,292
<   kf = KFold(n_splits = num_fold, random_state = rand_state, shuffle = True)
<   all_val_loss = np.zeros((num_epochs,))
<   for ind_fold, (train_pat_index, test_pat_index) in enumerate(kf.split(np.arange(num_pat))):
<     #===================================
<     # clear
<     #===================================
<     X_train = None
<     X_test = None
<     Y_train = None
<     Y_test = None
<     Y_train_loc = None
<     Y_test_loc = None
<     Y_train_mask = None
<     Y_test_mask = None
<     model = None
<     checkpoint = None
<     train_history = None
<     csv_logger = None
<     K.clear_session();
<     train_dataset = None
<     val_dataset = None
<     gc.collect();
< 
<     #===================================
<     # set up fold data
<     #===================================
<     #print("TRAIN:", train_pat_index, "TEST:", test_pat_index)
<     print('='*80)
<     print(f'FOLD: {ind_fold+1}/{num_fold}')
<     print('='*80)
<     train_index, test_index = get_indices(train_pat_index, test_pat_index, num_data_dict, loc_arr, ext_arr, sev_arr)
<     X_train, X_test = X_data[train_index], X_data[test_index]
<     Y_train, Y_test = Y_data[train_index], Y_data[test_index]
<     Y_train_loc, Y_test_loc = Y_data_loc[train_index], Y_data_loc[test_index]
<     Y_train_mask, Y_test_mask = Y_data_mask[train_index], Y_data_mask[test_index]
<     print("--------------------------------------------------------")
<     print("Training Input Shape => " + str(X_train.shape))
<     print("Testing Input Shape => " + str(X_test.shape))
---
>   #kf = KFold(n_splits = num_fold, random_state = rand_state, shuffle = True)
>   #for ind_fold, (train_pat_index, test_pat_index) in enumerate(kf.split(np.arange(num_pat))):
>   #===================================
>   # clear
>   #===================================
>   X_train = None
>   X_test = None
>   Y_train = None
>   Y_test = None
>   Y_train_loc = None
>   Y_test_loc = None
>   model = None
>   checkpoint = None
>   train_history = None
>   csv_logger = None
>   K.clear_session();
>   gc.collect();
> 
>   #===================================
>   # set up fold data
>   #===================================
>   print("--------------------------------------------------------")
>   print("Training Input Shape => " + str(X_data.shape))
>   print("Testing Input Shape => " + str(X_data.shape))
> 
>   train_dataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data, Y_data_loc, Y_data_mask))
>   train_dataset = train_dataset.shuffle(buffer_size=len(X_data)).batch(bt_size)
>   val_dataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data, Y_data_loc, Y_data_mask)).batch(bt_size)
304,349d293
<     train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train, Y_train_loc, Y_train_mask))
<     train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(bt_size)
<     val_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test, Y_test_loc, Y_test_mask)).batch(bt_size)
<     
< 
<     #===================================
<     # custom loop
<     #===================================
<     denoise_obs_net = build_dennet3D(input_shape, loc_shape, num_reg, lambda_val_chdiff, lambda_val_mdiff, U)
<     com_flag = f'_d{dose_level}_it{num_iter}_b{bt_size}_lmbdchdiff{lambda_val_ind_chdiff}_lmbdmdiff{lambda_val_ind_mdiff}_f{ind_fold}'
<     weights_base_name = f'{mod_data_folder}/weights/{weights_name}' + com_flag
<     opt = tf.keras.optimizers.Adam(learning_rate=1e-3) #change
< 
<     #print(f'tf.executing_eagerly(): {tf.executing_eagerly()}')
<     logs = {}
<     logs_it = {}
<     val_flag = 1
< 
<     for epoch in range(num_epochs):
<       start_time = time.time()
<       print(f'Epoch:{epoch + 1}/{num_epochs}')
<       #=====================================================================
<       # step-1: Training steps
<       #=====================================================================
<       denoise_obs_net.reset_metrics()
<       loss_value = 0
<       for step, (x_train_batch, y_train_batch, y_train_loc_batch, y_train_mask_batch) in enumerate(train_dataset):
<         # step-1: Do a training step fo each minibatch=>
<         # forward run + auto-differnetiate + backprop update
<         # loss_value += train_step(denoise_obs_net, x_train_batch, y_train_batch, opt)
<         cur_loss = train_step(denoise_obs_net, x_train_batch, y_train_batch, y_train_loc_batch, y_train_mask_batch, opt)
<         loss_value += cur_loss
<         for m in denoise_obs_net.metrics:
<           if 'train_' + m.name in logs_it:
<             logs_it['train_' + m.name].append(m.result())
<           else:
<             logs_it['train_' + m.name] = [m.result()]
< 
<       #=====================================================================
<       # step-2: log all losses and metrics on each epoch end of training set
<       #=====================================================================
<       for m in denoise_obs_net.metrics:
<         if 'train_' + m.name in logs:
<           logs['train_' + m.name].append(m.result())
<         else:
<           logs['train_' + m.name] = [m.result()]
351,355c295,301
<       cur_train_loss = loss_value / (step + 1)
<       if 'train_loss' in logs:
<         logs['train_loss'].append(cur_train_loss)
<       else:
<         logs['train_loss'] = [cur_train_loss]
---
>   #===================================
>   # custom loop
>   #===================================
>   denoise_obs_net = build_dennet3D(input_shape, loc_shape, num_reg, lambda_val_chdiff, lambda_val_mdiff, U)
>   com_flag = f'_d{dose_level}_it{num_iter}_b{bt_size}_lmbdchdiff{lambda_val_ind_chdiff}_lmbdmdiff{lambda_val_ind_mdiff}'
>   weights_base_name = f'{mod_data_folder}/weights/{weights_name}' + com_flag
>   opt = tf.keras.optimizers.Adam(learning_rate=1e-3) #change
357,387c303,323
<       #=====================================================================
<       # step-3: log metric in terminal
<       #=====================================================================
<       print(
<               f'Train: loss={cur_train_loss:.3f} || ' \
<               f"mse = {logs['train_mse'][-1]:.3f} || " \
<               f"chv_mse = {logs['train_chv_mse'][-1]:.3f} || " \
<               f"masked_diff = {logs['train_masked_diff'][-1]:.3f} || " \
<           )
< 
<       if val_flag:
<         #=====================================================================
<         # step-3: Validation steps
<         #=====================================================================
<         denoise_obs_net.reset_metrics()
<         loss_value = 0
<         for step, (x_batch_val, y_batch_val, y_batch_loc_val, y_batch_mask_val) in enumerate(val_dataset):
<           loss_value += val_step(denoise_obs_net, x_batch_val, y_batch_val, y_batch_loc_val, y_batch_mask_val)
< 
<         #=====================================================================
<         # step-4: log all losses and metrics on each epoch end of validation set
<         #=====================================================================
<         for m in denoise_obs_net.metrics:
<           if 'val_' + m.name in logs:
<             logs['val_' + m.name].append(m.result())
<           else:
<             logs['val_' + m.name] = [m.result()]
< 
<         cur_val_loss = loss_value / (step + 1)
<         if 'val_loss' in logs:
<           logs['val_loss'].append(cur_val_loss)
---
>   logs = {}
>   logs_it = {}
>   val_flag = 1
> 
>   for epoch in range(num_epochs):
>     start_time = time.time()
>     print(f'Epoch:{epoch + 1}/{num_epochs}')
>     #=====================================================================
>     # step-1: Training steps
>     #=====================================================================
>     denoise_obs_net.reset_metrics()
>     loss_value = 0
>     for step, (x_train_batch, y_train_batch, y_train_loc_batch, y_train_mask_batch) in enumerate(train_dataset):
>       # step-1: Do a training step fo each minibatch=>
>       # forward run + auto-differnetiate + backprop update
>       # loss_value += train_step(denoise_obs_net, x_train_batch, y_train_batch, opt)
>       cur_loss = train_step(denoise_obs_net, x_train_batch, y_train_batch, y_train_loc_batch, y_train_mask_batch, opt)
>       loss_value += cur_loss
>       for m in denoise_obs_net.metrics:
>         if 'train_' + m.name in logs_it:
>           logs_it['train_' + m.name].append(m.result())
389c325
<           logs['val_loss'] = [cur_val_loss]
---
>           logs_it['train_' + m.name] = [m.result()]
391,459c327,334
<       #=====================================================================
<       # step-5: log metric in terminal
<       #=====================================================================
<       if val_flag:
<         print(
<               f'Val  : loss={cur_val_loss  :.3f} || ' \
<               f"mse = {logs['val_mse'][-1]:.3f} || " \
<               f"chv_mse = {logs['val_chv_mse'][-1]:.3f} || " \
<               f"masked_diff = {logs['val_masked_diff'][-1]:.3f} || " \
<           )
<         print(f'Elapsed time: {(time.time() - start_time):.2f}s')
<       print('=' * 60)
< 
<       #=====================================================================
<       # step-6: save model, relevant metrics and loss function
<       #=====================================================================
<       save_freq = 20 #change
<       if (epoch + 1) % num_epochs == 0:
<         weights_filename = f'{weights_base_name}_ep{epoch+1:03d}.hdf5'
<         denoise_obs_net.save_weights(
<                             weights_filename,
<                             save_format='h5',
<                           )
<     #-----------------------------------------------------------
<     # Step-3: Save model and loss curve
<     #----------------------------------------------------------
<     # train and validation losses
<     train_loss = logs['train_loss']
<     val_loss = logs['val_loss']
<     print(f'Val_loss:{val_loss}')
<     all_val_loss += val_loss
< 
<     # train and validation metrics
<     # MSE
<     train_mse = logs['train_mse']
<     val_mse = logs['val_mse']
< 
<     # chv_mse
<     train_chv_mse = logs['train_chv_mse']
<     val_chv_mse = logs['val_chv_mse']
< 
<     # masked_diff
<     train_masked_diff = logs['train_masked_diff']
<     val_masked_diff = logs['val_masked_diff']
< 
<     mdict = {
<               "train_loss": train_loss, "val_loss": val_loss, 
<               "train_mse": train_mse, "val_mse": val_mse,
<               "train_chv_mse": train_chv_mse, "val_chv_mse": val_chv_mse,
<               "train_masked_diff": train_masked_diff, "val_masked_diff": val_masked_diff,
<             }
<     loss_curve_name = loss_fn_name + com_flag
< 
<     sio.savemat(f'{mod_data_folder}/losses/{loss_curve_name}.mat', mdict)
<     np.save(f'{mod_data_folder}/losses/{loss_curve_name}.npy', mdict)
< 
<     sio.savemat(f'{mod_data_folder}/losses/{loss_curve_name}_it.mat', logs_it)
<     #-----------------------------------------------------------
<     # Step-4: Plot loss and metric curve
<     #----------------------------------------------------------
<     # loss
<     png_filename = f'{mod_data_folder}/losses/{loss_curve_name}.png'
<     log_plot_loss_metric(
<                           train_loss, val_loss, 
<                           'Train Loss', 'Validation Loss', 
<                           'Loss',
<                           'Loss Curve',
<                           png_filename
<                         )
---
>     #=====================================================================
>     # step-2: log all losses and metrics on each epoch end of training set
>     #=====================================================================
>     for m in denoise_obs_net.metrics:
>       if 'train_' + m.name in logs:
>         logs['train_' + m.name].append(m.result())
>       else:
>         logs['train_' + m.name] = [m.result()]
461,477c336,362
<     # mse
<     png_filename = f'{mod_data_folder}/losses/{loss_curve_name}_mse.png'
<     log_plot_loss_metric(
<                           train_mse, val_mse, 
<                           'Train MSE', 'Validation MSE', 
<                           'MSE',
<                           'MSE Curve',
<                           png_filename
<                         )
<     # chv_mse
<     png_filename = f'{mod_data_folder}/losses/{loss_curve_name}_chv_mse.png'
<     log_plot_loss_metric(
<                           train_chv_mse, val_chv_mse, 
<                           'Train CHV MSE', 'Validation CHV MSE', 
<                           'CHV_MSE',
<                           'CHV_MSE Curve',
<                           png_filename
---
>     cur_train_loss = loss_value / (step + 1)
>     if 'train_loss' in logs:
>       logs['train_loss'].append(cur_train_loss)
>     else:
>       logs['train_loss'] = [cur_train_loss]
> 
>     #=====================================================================
>     # step-3: log metric in terminal
>     #=====================================================================
>     print(
>             f'Train: loss={cur_train_loss:.3f} || ' \
>             f"mse = {logs['train_mse'][-1]:.3f} || " \
>             f"chv_mse = {logs['train_chv_mse'][-1]:.3f} || " \
>             f"masked_diff = {logs['train_masked_diff'][-1]:.3f} || " \
>         )
> 
>     print('=' * 60)
> 
>     #=====================================================================
>     # step-6: save model, relevant metrics and loss function
>     #=====================================================================
>     save_freq = 20 #change
>     if (epoch + 1) % num_epochs == 0:
>       weights_filename = f'{weights_base_name}_ep{epoch+1:03d}.hdf5'
>       denoise_obs_net.save_weights(
>                           weights_filename,
>                           save_format='h5',
479,490d363
<     # masked_diff
<     png_filename = f'{mod_data_folder}/losses/{loss_curve_name}_masked_diff.png'
<     log_plot_loss_metric(
<                           train_masked_diff, val_masked_diff, 
<                           'Train MASKED DIFF', 'Validation MASKED DIFF', 
<                           'MASKED_DIFF',
<                           'MASKED_DIFF Curve',
<                           png_filename
<                         )
< 
< 
< 
492c365
<   # save best validation loss epoch
---
>   # Step-2: Build and Train Model
494,499c367,373
<   print(f'All_val_loss: {all_val_loss}')
<   all_fold_min_val_loss_ind = int(np.argmin(all_val_loss) + 1)
<   print(f'Best Epoch: {all_fold_min_val_loss_ind}')
<   com_flag = f'_d{dose_level}_it{num_iter}_b{bt_size}_lmbdchdiff{lambda_val_ind_chdiff}_lmbdmdiff{lambda_val_ind_mdiff}'
<   fname = f'{mod_data_folder}/losses/best_epoch_{com_flag}'
<   np.savetxt(f'{fname}.txt',  np.atleast_1d(all_fold_min_val_loss_ind), fmt='%d')
---
>   # Function Call: def build_segnet3D_par(input_shape,num_GPUs,num_reg):
>   #parallel_model = build_segnet3D_par(input_shape,num_GPUs,num_reg)
>   #-----------------------------------------------------------
>   # Step-3: Save model and loss curve
>   #----------------------------------------------------------
>   # train and validation losses
>   train_loss = logs['train_loss']
500a375,396
>   # train and validation metrics
>   # MSE
>   train_mse = logs['train_mse']
> 
>   # chv_mse
>   train_chv_mse = logs['train_chv_mse']
> 
>   # masked_diff
>   train_masked_diff = logs['train_masked_diff']
> 
>   mdict = {
>             "train_loss": train_loss,
>             "train_mse": train_mse,
>             "train_chv_mse": train_chv_mse,
>             "train_masked_diff": train_masked_diff,
>           }
>   loss_curve_name = loss_fn_name + com_flag
> 
>   sio.savemat(f'{mod_data_folder}/losses/{loss_curve_name}.mat', mdict)
>   np.save(f'{mod_data_folder}/losses/{loss_curve_name}.npy', mdict)
> 
>   sio.savemat(f'{mod_data_folder}/losses/{loss_curve_name}_it.mat', logs_it)
523a420,422
> 
> 
> 
527,528c426
<   train_seg3D(args.base_folder, args.weights_name, args.loss_fn_name, args.dose_level, args.num_iter, 
<                 args.batch_size, args.epochs, args.learning_folder, 
---
>   train_seg3D(args.base_folder, args.weights_name, args.loss_fn_name, args.dose_level, args.num_iter, args.batch_size, args.epochs, args.learning_folder,
530a429
> 
